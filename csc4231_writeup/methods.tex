\section{Methods}

\subsection{CRF vs HMM}
The HMM in \citep{rampasek2014fcnv} is a generative model whose discriminative model pair is a linear-chain conditional random field (CRF). This is a natural model to compare against the HMM and, as we discuss in this section, is possibly a more appropriate model for the task. Because using CRFs may require training for feature weights we consider what improvements can be gained and problems may be introduced by training a discriminative model.

The HMM defines a joint model $p(\cnv) p(\snp | \cnv)$ of the CNV inheritance pattern $\cnv$ and SNP allelic ratios $\snp$. Yet the task is to predict the CNVs of the fetal genome after having observed allelic ratios at distinct SNP positions of the sequenced maternal plasma cfDNA. This is done by computing the most likely path in the conditional distribution $p(\cnv | \snp)$ that the HMM implicitly defines. This suggests directly modelling $p(\cnv | \snp)$ without wasting model capacity on the marginal of the allelic ratios, $p(\snp)$. The hypothesis space of conditional distributions of HMMs corresponds exactly to a family of conditional Markov random fields with special pairwise potentials known as linear-chain CRFs \citep{sutton2012}. In other words, any conditional distribution $p(\cnv | \snp)$ of an HMM can be converted into a linear-chain CRF with the same distributions. This also means that there exist inference algorithms for the linear-chain CRFs that are exact in poly time, and since the model is fully observed the likelihood function is convex and maximum likelihood training will converge to the global optimum.

The primary advantage of discriminative models is that they avoid paying any cost for complex structure in the input. Although the conditional distribution of HMMs and a particular class of linear-chain CRFs define the same hypothesis space, the HMM training is biased towards models that spend capacity on modelling the marginal of the SNP allelic ratios $p(\snp)$. More general linear-chain CRFs are more expressive and can include rich, dependent features of the inputs without requiring more complex inference algorithms or inducing intractability. 

There are some possible disadvantages of linear-chain CRFs for CNV data. First, and perhaps most important, it is not clear how to proceed with training in the absence of real data sets. Second, if only small datasets were available it is possible that the HMM training procedure results in better conditional distributions. This is because the marginal over the SNPs $p(\snp)$ can have a smoothing effect on the resulting conditional distribution. In general discriminative models are more prone to overfitting. Yet, discriminative models are a natural direction for further research and the presence of \textit{in silico} CNV simulation methods developed in \citep{rampasek2014fcnv} make training linear-chain CRFs a promising direction for CNV prediction. Linear-chain CRFs enjoy widespread success in NLP for tasks like named-entity recognition, shallow parsing, semantic role finding, and word alignment in machine translation \citep{sutton2012}. Many of these tasks share structure with the CNV prediction task.  Indeed, semi-Markov CRFs have been used in computational biology for tasks like gene prediction \citep{bernal2007} and RNA structural alignment \citep{Sato01012005}.


%%%%%%%%%%%%%%%%%% FIGURE TEMPLATES %%%%%%%%%%%%%%%%%
%%%%%%%%% SINGLE FIGURE %%%%%%%%
%\begin{figure}
%\caption{caption fdfd}
%\label{fig:}
%\centering
%\includegraphics[height=0.33\textheight]{figures/}
%\end{figure}
%
%%%%%$%%%% MULTIFIGURE %%%%%%%%%
%\begin{figure*}
%\caption{caption fdfd}
%\label{fig:}
%\subfigure[subfig title A]{ 
%\begin{minipage}[b]{0.48\textwidth}
%	\centering
%	\includegraphics[width=0.98\textwidth]{figures/}
%	\end{minipage}	
%}
%\subfigure[subfig title B]{
%	\begin{minipage}[b]{0.48\textwidth}
%		\centering
%	\includegraphics[width=0.98\textwidth]{figures/}
%	\end{minipage}	
%}
%\end{figure*}
