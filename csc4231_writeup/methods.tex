\section{Methods}
The task of CNV detection can be formalized as a sequence segmentation task---given a sequence of allele counts (reference and variant counts) $\ar_{\pos}$ at SNP location $\pos$ predict the phased inheritance pattern of the feature $\pip_{\pos}$ for that SNP. Thus, the segmentation problem is predicting the class of each $\pip_{\pos}$ from $\ars$. In this section we describe two related models (CRFs and HMMs) for solving this problem and introduce the CRF architecture explored in this paper.

\subsection{CRF vs HMM}
An HMM is a natural model for sequence segmentation. HMMs model the distribution of $\pips$ and $\ars$ as a generative model with a particular first-order Markov structure:
\begin{align*}
p(\ars, \pips) = p(\pip_1) p(\ar_1 | \pip_1) \prod_{\pos > 1} p(\pip_{\pos} | \pip_{\pos-1})  p(\ar_{\pos} | \pip_{\pos})
\end{align*}
Traditionally the HMM is trained to optimize the probability of producing the data $\ars$. Yet the task is to predict the most likely inheritance pattern. This is done by computing the most likely path in the conditional distribution $p(\pips | \ars)$ that the HMM implicitly defines. 
\begin{align*}
\pips^* = \argmax_{\pips} p(\pips | \ars)
\end{align*}
This suggests directly modelling $p(\pips | \ars)$ without wasting model capacity on the marginal of the allelic ratios, $p(\ars)$. The hypothesis space of conditional distributions of HMMs corresponds exactly to a family of conditional Markov random fields with special pairwise potentials known as linear-chain CRFs \citep{sutton2012}. In other words, any conditional distribution $p(\pips | \ars)$ of an HMM can be converted into a linear-chain CRF with the same distributions. Linear chain CRFs usually parameterize the probability of segmentations as follows:
\begin{align*}
p(\pips | \ars) \propto \exp\left( \sum_{\pos} \sum_{k} w_k f_k(\pip_{\pos-1}, \pip_{\pos}, \ar_{\pos})\right)
\end{align*}
where the functions $f_k$ are real valued functions called feature functions and $w_k$ are weights, respectively, for the feature functions. If the observations $\ars$ have a categorical distribution with possible states $m$, the linear-chain CRF that corresponds to the conditional distribution over labelings defined by an HMM has features of the following form
\begin{align*}
&\indicator{\pip_{\pos} = s, \ar_{\pos} = m}\\
&\indicator{\pip_{\pos-1} = s^{\prime} , \pip_{\pos} = s}
\end{align*}
where $\indicator{\text{condition}}$ is an indicator and $s,s^{\prime}$ are possible label states and $m$ is an emission state. To replicate the conditional of the HMM, we just need to set their respective weights to
\begin{align*}
&\log p(\ar_{\pos} = m | \pip_{\pos} = s)\\
&\log p( \pip_{\pos} = s | \pip_{\pos-1} = s^{\prime})
\end{align*}
The polytime inference algorithms for HMMs have exact analogues for linear-chain CRFs, and since the model is fully observed the likelihood function is convex and maximum likelihood training will converge to the global optimum. Thus, CRFs are a natural model to compare against the HMM and, as we discuss in this section, is possibly a more appropriate model for the task.

The primary advantage of discriminative models is that they avoid paying any cost for complex structure in the input. Although the conditional distribution of HMMs and a particular class of linear-chain CRFs define the same hypothesis space, the HMM training is biased towards models that spend capacity on modelling the marginal of the SNP allelic ratios $p(\ars)$. Moreover, there is no need to restrict the feature functions dependence on $\ars$. In particular the feature functions can include rich, dependent features of the inputs without requiring more complex inference algorithms or inducing intractability. Furthermore, similar to HMMs we can have the feature functions depend on the position $\pos$. So, we can rewrite the family of linear-chain CRFs as distributions with the following form
\begin{align*}
p(\pips | \ars) \propto \exp\left( \sum_{\pos} \sum_{k} w_k f_{k}(\pip_{\pos-1}, \pip_{\pos}, \ars, \pos)\right) \equiv \exp(S_{\bm{w}}(\ars, \pips))
\end{align*}

There are some possible disadvantages of linear-chain CRFs for CNV data. First, and perhaps most important, it is not clear how to proceed with training in the absence of real data sets. Second, if only small datasets were available it is possible that the HMM training procedure results in better conditional distributions. This is because the marginal over the SNPs $p(\ars)$ can have a smoothing effect on the resulting conditional distribution. In general discriminative models are more prone to overfitting. Yet, discriminative models are a natural direction for further research and the presence of \textit{in silico} CNV simulation methods developed in \cite{rampasek2014fcnv} make training linear-chain CRFs a promising direction for CNV prediction. Linear-chain CRFs enjoy widespread success in NLP for tasks like named-entity recognition, shallow parsing, semantic role finding, and word alignment in machine translation \citep{sutton2012}. Many of these tasks share structure with the CNV prediction task.  Indeed, semi-Markov CRFs have been used in computational biology for tasks like gene prediction \citep{bernal2007} and RNA structural alignment \citep{Sato01012005}.

\subsection{Description of the CRF Model}
In this section we describe the feature functions of our CRF. We consider two types of weights: node features and edge features; node features describe the dependence of $\pip_{\pos}$ on $\ar_{\pos}$, and edge features describe the pairwise dependencies between $\pip_{\pos}$ on $\pip_{\pos - 1}$.

\subsubsection{Node Feature: Beta-binomial Features}
The Beta-binomial features is the result of inverting a Beta-binomial emission distribution in an HMM. The Beta-binomial is parameterized by $\alpha$ and $\beta$, the parameters of the beta, so we have a separate $\alpha_s$ and $\beta_s$ for every possible phased inheritance pattern. In particular let $N_{\pos}$ be the number of reads aligned to position $\pos$, then $\ar_{\pos}$ is a tuple $(\refe_{\pos}, \alte_{\pos})$ of reference and variant counts that sum to $N_{\pos}$. The following generative model describes generating $\refe_{\pos}$ from a Beta-binomial
\begin{align*}
p \sim Beta(\alpha_s, \beta_s)\\
\refe_{\pos} \sim Binomial(N_{\pos}, p)
\end{align*}
Under this model the log probability of generating $\refe_{\pos}$ is
\begin{align*}
\log p(\ar_{\pos} | \pip_{\pos} = s) = \log \left(\binom{N_{\pos}}{\refe_{\pos}} \frac{B(\refe_{\pos} + \alpha_s, N_{\pos} - \refe_{\pos} + \beta_s)}{B(\alpha_s, \beta_s)}\right)
\end{align*}
where $B(x,y)$ is the beta function. Thus we include a feature of the form
\begin{align*}
\indicator{\pip_{\pos} = s}\log \left(\binom{N_{\pos}}{\refe_{\pos}} \frac{B(\refe_{\pos} + \alpha_s, N_{\pos} - \refe_{\pos} + \beta_s)}{B(\alpha_s, \beta_s)}\right) 
\end{align*}
for every possible phased inheritance pattern. The Beta-binomial can be reparameterized in terms of a location $\mu_s$ and an over-dispersion parameter $\rho_s$. Each phased inheritance pattern predicts a certain proportion of reference vs. alternative alleles (as described in \cite{rampasek2014fcnv}), we set $\mu_s$ to this expected value. The over-dispersion parameter $\rho_s = \rho$ is shared across all phased inheritance patterns and was estimated from the entire training set of allelic ratios using the method of moments estimator from \citet{weirHill2002}. There is a single weight $w_{BB}$ shared across all positions and states that weights the contribution of the Beta-binomial feature.

\subsubsection{Node Feature: Coverage Feature}
Depth of coverage information provides a prediction for the number of parental haplotypes inherited by the fetus. We adopt the method in \citet{rampasek2014fcnv}  for computing the probability of fetal haplotype copy counts. Thus for each phased inheritance pattern $s$, we can compute its copy count $\copycount(s) \in \{0, 1, 2\}$, and then the probability of that copy count conditioned on the depth of coverage at position $\pos$. Thus, we add features
\begin{align*}
\indicator{\pip_{\pos} = s}\log \frac{p(\copycount(s) = k | doc_{\pos})}{\text{\# of phased inheritance patterns $s$ with } \copycount(s) = k}
\end{align*}
Thus, with no other features these features would predict copy counts with the right probabilities.There is a single weight $w_{DOC}$ shared across all positions and states that weights the contribution of the coverage feature.

\subsubsection{Edge Features}
The distance in base pairs between two adjacent SNP locations can provide information about the probability of a recombination event or CNV. The traditional edge features would be $\indicator{\pip_{\pos-1} = s^{\prime} , \pip_{\pos} = s}$. We incorporate into these traditional edge features information about the distance between $\pip_{\pos-1} = s^{\prime}$ and $\pip_{\pos} = s$. The information is incorporated by first binning all distances into the following 5 bins: 
\begin{align*}
< 120 \; bp\\
< 320 \; bp\\
< 730 \; bp\\
< 3 \; kbp\\
\geq 3 \; kbp
\end{align*}
These distance bins were empirically estimated from SNP positions in chromosome 1, where $3\%$ of the adjacent SNPs are $\geq 3\;kbp$ apart. The other distance bins brake points correspond to quartiles of the empirical distribution for distances that are $< 3\;kbp$.
This binning allows us to define the following edge features:
\begin{align*}
\indicator{\pip_{\pos-1} = s^{\prime} , \pip_{\pos} = s, bin(dist(\pip_{\pos-1} , \pip_{\pos} )) = k}
\end{align*}
Thus, there are potentially $5 \times $ \# phased inheritance patterns $\times $ \# phased inheritance many weights. However, we restrict most of the weights to be negative infinity (this has the effect of making certain transition impossible). We do this in order to replicate the connectivity of the HMM reported in \citet{rampasek2014fcnv}. This results $5 \times 6$ finite weights allowing only the transitions from \citet{rampasek2014fcnv}.

\subsection{Training CRFs}

Because CRFs have tractable inference it is common to simply train the model by following the gradient of the log likelihood. For CRFs the gradient on a single data point $\ars, \pips$ has a particularly nice form leading to the following weight update:
\begin{align*}
\Delta w_k = \tau \left(\sum_{\pos} f_k(\pip_{\pos-1}, \pip_{\pos}, \ars, \pos) - \sum_{\pos} \mathbb{E} [f_k(\pip_{\pos-1}, \pip_{\pos}, \ars, \pos) | \ars]\right)
\end{align*}
where $\mathbb{E} [ \cdot | \ars]$ is the expected value over the labelings with respect to the current weights and $\tau$ is a learning rate. These expected values can be computed with CRF analogues of the HMM forward and backward algorithms. 

Margin Infused Relaxed Algorithm (MIRA) is an alternative algorithm for training CRFs that is in the family of large-margin learning methods. This type of training is usually less precise but much faster than the maximum likelihood training. Large-margin learning methods were originally developed in the context of Support Vector Machines (SVM), which classify with a separating hyperplane. The observation in the context of SVM training is that separating hyperplanes with a large margin to training points is more robust to noise and leads to better prediction. MIRA is an algorithm in this spirit that extends this intuition to a more general class of models \citep{crammerThesis}. MIRA was used to train CRFs in a gene prediction task \citep{bernal2007}. The weight update rule is simple
\begin{align*}
\Delta w_k = \tau \left(\sum_{\pos} f_k(\pip_{\pos-1}, \pip_{\pos}, \ars, \pos) - \sum_{\pos} f_k(\pip^*_{\pos-1}, \pip^*_{\pos}, \ars, \pos)\right)
\end{align*}
where $\pips^* = \argmax_{y} p(\pips|\ars)$ for the current model distribution. The specifics of the MIRA algorithm are in how the learning rate $\tau$ is set. In the version of MIRA we used $\tau$ is set
\begin{align*}
\tau = \min\left(C, \frac{S_{\bm{w}}(\ars, \pips^*) - S_{\bm{w}}(\ars, \pips) + \mathcal{L}(\pips, \pips^*)}{\left(\sum_{\pos} \sum_k (f_k(\pip_{\pos-1}, \pip_{\pos}, \ars, \pos) - f_k(\pip^*_{\pos-1}, \pip^*_{\pos}, \ars, \pos))^2 \right)^2}\right)
\end{align*}
where $C$ is a user-specified constant and $\mathcal{L}(\pips, \pips^*)$ is any non-negative loss function. In this work we used a loss function that penalizes for different types of misclassifications. In particular, if $C_{ss^{\prime}}$ is the number of ground truth $\pip_{\pos} = s$ that were classified as $s^{\prime}$, then our loss function was
\begin{align*}
\mathcal{L}(\pips, \pips^*) = \sum_{s, s^{\prime}} b_{ss^{\prime}} C_{ss^{\prime}}
\end{align*}
We set the parameters $b_{ss^{\prime}}$ according to significance of individual misclassification types, e.g. misclassification within an inheritance pattern (that is getting the phasing wrong, but correct inheritance pattern) was penalized only relatively lightly.


%%%%%%%%%%%%%%%%%% FIGURE TEMPLATES %%%%%%%%%%%%%%%%%
%%%%%%%%% SINGLE FIGURE %%%%%%%%
%\begin{figure}
%\caption{caption fdfd}
%\label{fig:}
%\centering
%\includegraphics[height=0.33\textheight]{figures/}
%\end{figure}
%
%%%%%$%%%% MULTIFIGURE %%%%%%%%%
%\begin{figure*}
%\caption{caption fdfd}
%\label{fig:}
%\subfigure[subfig title A]{ 
%\begin{minipage}[b]{0.48\textwidth}
%	\centering
%	\includegraphics[width=0.98\textwidth]{figures/}
%	\end{minipage}	
%
%\subfigure[subfig title B]{
%	\begin{minipage}[b]{0.48\textwidth}
%		\centering
%	\includegraphics[width=0.98\textwidth]{figures/}
%	\end{minipage}	
%}
%\end{figure*}